{
 "cells": [
  {
   "cell_type": "code",
   "id": "ebbea2e0-931d-4174-9fed-2f309348dff6",
   "metadata": {},
   "source": [
    "%pip install elasticsearch\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "id": "60c457a0-f0d0-4734-be27-dcfc8d8e9d45",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from elasticsearch import Elasticsearch\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, StringType  # Import StringType here\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My App\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./work/data.csv\")\n",
    "\n",
    "# Convert label column to numeric\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Drop the original string label column\n",
    "data = data.drop(\"label\")\n",
    "\n",
    "# Rename the new numeric label column to \"label\"\n",
    "data = data.withColumnRenamed(\"label_index\", \"label\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "data = tokenizer.transform(data)\n",
    "\n",
    "# Create TF-IDF features\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[hashingTF, idf])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "data = pipelineFit.transform(data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, probabilityCol=\"probability\")\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "# Extract the probability for the positive class from the \"probability\" column\n",
    "def get_positive_probability(probability):\n",
    "    return probability[1].item()\n",
    "\n",
    "get_positive_probability_udf = udf(get_positive_probability, DoubleType())\n",
    "\n",
    "predictions = predictions.withColumn(\"positive_probability\", get_positive_probability_udf(predictions[\"probability\"]))\n",
    "\n",
    "# Define a UDF to convert sentiment probability to label\n",
    "def get_sentiment_label(probability):\n",
    "    if probability >= 0.5:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "sentiment_udf = udf(get_sentiment_label, StringType())\n",
    "\n",
    "# Add a column for sentiment label based on the positive probability\n",
    "predictions = predictions.withColumn(\"sentiment\", sentiment_udf(predictions[\"positive_probability\"]))\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(['http://elasticsearch:9200'])\n",
    "\n",
    "# Check if the connection is successful\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Could not connect to Elasticsearch\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "predictions_pandas = predictions.select(\"id\", \"sentiment\").toPandas()\n",
    "\n",
    "# Save results to Elasticsearch\n",
    "for row in predictions_pandas.itertuples():\n",
    "    es.index(index='sentiment_analysis_index', body={'sentiment': row.sentiment, 'id': row.id})\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "pred = [1 if sentiment == 'positive' else 0 for sentiment in predictions_pandas['sentiment']]\n",
    "actual = [1 if label == 'pos' else 0 for label in testData.select('label').collect()]\n",
    "print(classification_report(actual, pred, target_names=['negative', 'positive']))\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
